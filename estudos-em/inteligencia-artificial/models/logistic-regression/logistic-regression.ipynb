{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "source": [
        "## Libraries and config"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: numpy in /home/tuca/.local/lib/python3.8/site-packages (1.19.5)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pandas in /home/tuca/.local/lib/python3.8/site-packages (1.2.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas) (2019.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /home/tuca/.local/lib/python3.8/site-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas) (2.7.3)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: numpy in /home/tuca/.local/lib/python3.8/site-packages (1.19.5)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: matplotlib in /home/tuca/.local/lib/python3.8/site-packages (3.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /home/tuca/.local/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib) (7.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3/dist-packages (from matplotlib) (2.7.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/tuca/.local/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/tuca/.local/lib/python3.8/site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /home/tuca/.local/lib/python3.8/site-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: six in /home/tuca/.local/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: skll in /home/tuca/.local/lib/python3.8/site-packages (2.5.0)\n",
            "Requirement already satisfied: scipy in /home/tuca/.local/lib/python3.8/site-packages (from skll) (1.6.3)\n",
            "Requirement already satisfied: seaborn in /home/tuca/.local/lib/python3.8/site-packages (from skll) (0.11.1)\n",
            "Requirement already satisfied: tabulate in /home/tuca/.local/lib/python3.8/site-packages (from skll) (0.8.9)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/tuca/.local/lib/python3.8/site-packages (from skll) (4.9.3)\n",
            "Requirement already satisfied: numpy in /home/tuca/.local/lib/python3.8/site-packages (from skll) (1.19.5)\n",
            "Requirement already satisfied: ruamel.yaml in /home/tuca/.local/lib/python3.8/site-packages (from skll) (0.17.4)\n",
            "Requirement already satisfied: pandas in /home/tuca/.local/lib/python3.8/site-packages (from skll) (1.2.4)\n",
            "Requirement already satisfied: joblib in /home/tuca/.local/lib/python3.8/site-packages (from skll) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn<=0.24.2,>=0.24.1 in /home/tuca/.local/lib/python3.8/site-packages (from skll) (0.24.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/tuca/.local/lib/python3.8/site-packages (from scikit-learn<=0.24.2,>=0.24.1->skll) (2.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/tuca/.local/lib/python3.8/site-packages (from beautifulsoup4->skll) (2.2.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas->skll) (2019.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas->skll) (2.7.3)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in /home/tuca/.local/lib/python3.8/site-packages (from ruamel.yaml->skll) (0.2.2)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /home/tuca/.local/lib/python3.8/site-packages (from seaborn->skll) (3.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /home/tuca/.local/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn->skll) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/tuca/.local/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn->skll) (0.10.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib>=2.2->seaborn->skll) (7.0.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/tuca/.local/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn->skll) (1.3.1)\n",
            "Requirement already satisfied: six in /home/tuca/.local/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn->skll) (1.15.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: keras in /home/tuca/.local/lib/python3.8/site-packages (2.3.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /home/tuca/.local/lib/python3.8/site-packages (from keras) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/tuca/.local/lib/python3.8/site-packages (from keras) (1.1.2)\n",
            "Requirement already satisfied: h5py in /home/tuca/.local/lib/python3.8/site-packages (from keras) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from keras) (5.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /home/tuca/.local/lib/python3.8/site-packages (from keras) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /home/tuca/.local/lib/python3.8/site-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /home/tuca/.local/lib/python3.8/site-packages (from keras) (1.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install skll\n",
        "!pip install keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as sklm\n",
        "import skll.metrics as skllm\n",
        "import copy\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "source": [
        "## Utils"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "PATH = \"./\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_data(flow_interval):\n",
        "    path = f\"{PATH}dataset/dataset_flow_{flow_interval}.csv\"\n",
        "    print(PATH)\n",
        "    data = pd.read_csv(path, ';')\n",
        "    \n",
        "    data['Flow'].apply(int)\n",
        "    data['AveSpeed'].apply(float)\n",
        "    data['Density'].apply(float)\n",
        "    data['Sunday'].apply(int)\n",
        "    data['Monday'].apply(int)\n",
        "    data['Tuesday'].apply(int)\n",
        "    data['Wednesday'].apply(int)\n",
        "    data['Thursday'].apply(int)\n",
        "    data['Friday'].apply(int)\n",
        "    data['Saturday'].apply(int)\n",
        "      \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def store(obj, path, name):\n",
        "  with open(\"{0}{1}/{2}.json\".format(PATH, path, name), 'w') as json_file:\n",
        "    json.dump(obj, json_file, sort_keys=True, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_dataset(data, useB, n_steps, n_future):\n",
        "  \"\"\" Generate Dataset\n",
        "  \n",
        "  Generate a dataset provided a sequence. Reshape the sequence in rolling intervals from [samples, timesteps] into \n",
        "  [samples, timesteps, features] and split the sequence. The split the sequence in rolling intervals with a corresponding value \n",
        "  like the example bellow.\n",
        "\n",
        "  Ex: split_sequence([1, 2, 3, 4, 5], 3) #([[1, 2, 3], [2, 3, 4]], [4, 5])\n",
        "  \n",
        "  Arguments:\n",
        "    raw_seq: the sequence to reshape.\n",
        "    useB: if the dataset is more complex or not.\n",
        "    n_steps: size of the rolling interval\n",
        "    n_future: the distance to the interval the value should be.  \n",
        "  \"\"\"\n",
        "\n",
        "  sequence = np.array(data if useB else data['Flow'])\n",
        "\n",
        "  n = len(sequence)\n",
        "  X, Y = list(), list()\n",
        "\n",
        "  for i in range(n):\n",
        "    j = i + n_steps\n",
        "    k = j + n_future\n",
        "\n",
        "    if k >= n:\n",
        "      break\n",
        "\n",
        "    seq_x, seq_y = sequence[i:j], sequence[k]\n",
        "    X.append(seq_x)\t\n",
        "    Y.append(seq_y[0] if useB else seq_y)\n",
        "\n",
        "  X, Y = np.array(X), np.array(Y)\t\n",
        "  \n",
        "  if not useB:\n",
        "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "  return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate (expected, observed, times, name):\n",
        "  \"\"\" Evaluate Sessions\n",
        "  \n",
        "  Evaluate models by RMSE, NRMSE, MAE, HR, PRE. It will store the \n",
        "  results in a object and return it.\n",
        "  \n",
        "  Arguments:\n",
        "    expected: an array of expected instances of each \n",
        "      train&test session.\n",
        "    observed: an array of observed instances of each \n",
        "      train&test session.\n",
        "    times: an array of the time of each train&test session.\n",
        "    name: the name of the model\n",
        "  \"\"\"\n",
        "  n = len(expected)\n",
        "  flatten = lambda l : [i for sl in l for i in sl]\n",
        "  \n",
        "  # Make the arrays serializable\n",
        "  expected = list(map(list, expected))\n",
        "  observed = list(map(list, observed))\n",
        "  \n",
        "  for i in range(n):\n",
        "    expected[i] = list(map(float, expected[i]))\n",
        "    observed[i] = list(map(float, observed[i]))\n",
        "  \n",
        "  raw = evaluate_raw(expected, observed, times)\n",
        "  \n",
        "  #n_buckets = len(raw['PRE'])\n",
        "  #_pre = [[pre[i] for pre in raw['PRE']] for i in range(n_buckets)]\n",
        "  \n",
        "  eva = {\n",
        "    'TIME': int(sum(times)),\n",
        "    'RMSE': float(np.mean(raw['RMSE'])),\n",
        "    'MAE': float(np.mean(raw['MAE'])),\n",
        "    'Kappa': float(np.mean(raw['Kappa'])),\n",
        "    'HR': float(np.mean(raw['HR'])),\n",
        "    'has_negative': (min(flatten(observed)) < 0),\n",
        "    'raw': raw\n",
        "  }\n",
        "  \n",
        "  print(f\"\\n{name} Final Result:\")\n",
        "  print(f\"\\tTotal Time: {eva['TIME']}s\")\n",
        "  print(f\"\\tRMSE: {eva['RMSE']}\")\n",
        "  print(f\"\\tMAE: {eva['MAE']}\")\n",
        "  print(f\"\\tKappa: {eva['Kappa']}\")\n",
        "  print(f\"\\tHit Ratio: {eva['HR'] * 100}%\")\n",
        "    \n",
        "  return eva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_raw (expected, observed, times):\n",
        "  \"\"\" Evaluate Raw Sessions \n",
        "  \n",
        "  Evaluate each of the train&test sessions by RMSE, NRMSE, MAE, HR, PRE. \n",
        "  It will store the results in a object and return it.\n",
        "  \n",
        "  Arguments:\n",
        "    expected: an array of expected instances of each train&test session.\n",
        "    observed: an array of observed instances of each train&test session.\n",
        "    times: an array of the time of each train&test session.\n",
        "  \"\"\"\n",
        "  \n",
        "  n = len(expected)\n",
        "\n",
        "  for i in range(n):\n",
        "    observed[i] = [0 if np.isnan(o) else o for o in observed[i]]\n",
        "\n",
        "  for i in range(n):\n",
        "    observed[i] = [max(o, 0) for o in observed[i]]\n",
        "  \n",
        "  raw = {\n",
        "    'expected': expected,\n",
        "    'observed': observed,\n",
        "    'TIME': times,\n",
        "    'RMSE': [0] * n,\n",
        "    'MAE': [0] * n,\n",
        "    'Kappa': [0] * n,\n",
        "    'HR': [0] * n,\n",
        "  }\n",
        "  \n",
        "  for i in range(n):\n",
        "    Y = expected[i]\n",
        "    Y_hat = observed[i]\n",
        "    time = times[i]\n",
        "\n",
        "    raw['RMSE'][i] = np.sqrt(sklm.mean_squared_error(Y, Y_hat))\n",
        "    raw['MAE'][i] = sklm.mean_absolute_error(Y, Y_hat)\n",
        "    raw['Kappa'][i] = skllm.kappa(Y, Y_hat)\n",
        "    raw['HR'][i] = evaluate_precision_hit_ratio(Y, Y_hat)\n",
        "    \n",
        "    if VERBOSITY:\n",
        "      print(f\"({i+1}/{n}) Test Size: {len(Y)}, Time: {time}s\")\n",
        "      print(f\"\\tRMSE: {raw['RMSE'][i]}\")\n",
        "      print(f\"\\tMAE: {raw['MAE'][i]}\")\n",
        "      print(f\"\\tKappa: {raw['Kappa'][i]}\")\n",
        "      print(f\"\\tHit Ratio: {raw['HR'][i] * 100}%\")\n",
        "\n",
        "  return raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_precision_hit_ratio (Y, Y_hat):\n",
        "  \"\"\" Trend Prediction Ratio Calculation\n",
        "  \n",
        "  Calculates the ratio of up/down prediction.\n",
        "  \n",
        "  Arguments:\n",
        "    Y: the expected dataset.\n",
        "    Y_hat: the observed dataset.\n",
        "  \"\"\"\n",
        "  \n",
        "  cnt = 0\n",
        "  \n",
        "  for i in range(len(Y)):\n",
        "    if i < N_FUTURE:\n",
        "      continue\n",
        "      \n",
        "    exp = Y[i] - Y[i - N_FUTURE]\n",
        "    obs = Y_hat[i] - Y[i - N_FUTURE]\n",
        "    \n",
        "    if exp * obs > 0:\n",
        "      cnt += 1\n",
        "    \n",
        "  return cnt / len(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_prediction (Y, Y_hat, title):\n",
        "  \"\"\" Plot Prediction\n",
        "  \n",
        "  Plot the prediction (Flow x Time) of what was expected and what\n",
        "  was predicted.\n",
        "  \"\"\"\n",
        "\n",
        "  name = f\"{title}\"\n",
        "  path = f\"plots/prediction/{name}\"\n",
        "  \n",
        "  plt.plot(Y)\n",
        "  plt.plot(Y_hat)\n",
        "  plt.title(f\"Predição do Modelo {title}\")\n",
        "  plt.ylabel('Fluxo')\n",
        "  plt.xlabel('Tempo')\n",
        "  plt.legend(['esperado', 'observado'], loc='upper left')\n",
        "  plt.rcdefaults()\n",
        "  plt.savefig(path + \".png\", bbox_inches='tight')\n",
        "  plt.close('all')"
      ]
    },
    {
      "source": [
        "## Model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Parameters\n",
        "SEEABLE_PAST = 480 # in minutes\n",
        "PREDICT_IN_FUTURE = 60 # in minutes\n",
        "FLOW_INTERVAL = 150 # the interval size for each flow\n",
        "N_SPLITS = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Derivated Model Parameters\n",
        "N_STEPS = SEEABLE_PAST * 60 // FLOW_INTERVAL # the number of flows to see in the past\n",
        "N_FUTURE = PREDICT_IN_FUTURE * 60 // FLOW_INTERVAL # how much in the future we want to predict (0 = predict the flow on the next FLOW_INTERVAL minutes)\n",
        "DAY_SIZE = (24 * 60 * 60) // FLOW_INTERVAL  \n",
        "WEEK_SIZE = (7 * 24 * 60 * 60) // FLOW_INTERVAL\n",
        "VERBOSITY = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_dataset(lenX):\n",
        "    return lenX - (WEEK_SIZE*2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def logistic_regression(data, useB):\n",
        "    global result_data\n",
        "  \n",
        "    name = \"LR B\" if useB else \"LR A\"\n",
        "\n",
        "    expected, observed, times = [], [], []\n",
        "\n",
        "    X, Y = generate_dataset(data, useB, FLOW_INTERVAL, N_STEPS, N_FUTURE)\n",
        "    X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
        "\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    pointers = split_dataset(len(X), SET_SPLIT, TEST_SPLIT)\n",
        "  \n",
        "    for i, j, k in pointers:\n",
        "        start = time.time()\n",
        "            \n",
        "        model.fit(X[i:j], Y[i:j])\n",
        "            \n",
        "        expected.append(Y[j:k])\n",
        "        observed.append(model.predict(X[j:k]))\n",
        "        times.append(time.time() - start)\n",
        "    \n",
        "    result_data['results'][name] = evaluate(expected, observed, times, name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './dataset/dataset_flow_150.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b167b651b3f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLOW_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-ba27dab21278>\u001b[0m in \u001b[0;36mretrieve_data\u001b[0;34m(flow_interval)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{PATH}dataset/dataset_flow_{flow_interval}.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Flow'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/dataset_flow_150.csv'"
          ]
        }
      ],
      "source": [
        "global result_data\n",
        "  \n",
        "result_data = {\n",
        "    'results': {},\n",
        "    'meta': {\n",
        "      'SEEABLE_PAST': SEEABLE_PAST,\n",
        "      'PREDICT_IN_FUTURE': PREDICT_IN_FUTURE,\n",
        "      'FLOW_INTERVAL': FLOW_INTERVAL,\n",
        "      'N_SPLITS': N_SPLITS,\n",
        "    }\n",
        "}\n",
        "\n",
        "data = retrieve_data(FLOW_INTERVAL)\n",
        "\n",
        "logistic_regression(data, False)\n",
        "logistic_regression(data, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1/1) Test Size: 8063, Time: 0s\n\tRMSE: 6.069194007743126\n\tMAE: 4.6463469482305015\n\tHit Ratio: 56.54222993922857%\n\nRNN-150 Final Result:\n\tTotal Time: 0s\n\tRMSE: 6.069194007743126\n\tMAE: 4.6463469482305015\n\tHit Ratio: 56.54222993922857%\n"
          ]
        }
      ],
      "source": [
        "ev = evaluate([validation[1]], [prediction], [0], \"LR A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "store(ev, 'results', 'LR A')"
      ]
    }
  ]
}